# Ollama

## Qu'est-ce qu ollama
Ollama est une application d√©velopp√©e en Go ( Golang, est un langage de programmation open-source d√©velopp√© par Google ), bas√©e sur llama.cpp, qui simplifie l'utilisation des mod√®les de langage (LLM) sur une machine locale.

**Facilit√© d'utilisation** : Ollama rend l'acquisition et l'ex√©cution des mod√®les LLM tr√®s simples. Il adapte automatiquement les prompts (les questions ou les textes d'entr√©e) au format attendu par chaque mod√®le et charge les mod√®les √† la demande avec des commandes tr√®s simples.

**API REST** : Ollama offre une API REST qui permet d'ex√©cuter et d'interagir avec les mod√®les LLM. Cela signifie que vous pouvez facilement int√©grer Ollama dans vos applications en envoyant des requ√™tes HTTP pour obtenir des r√©ponses des mod√®les.

**Compatibilit√© multiplateforme** : Ollama fonctionne en t√¢che de fond et est disponible sur Mac OS, Linux et, r√©cemment, sur Windows en version preview. De plus, depuis octobre 2023, Ollama est int√©gr√© dans une image Docker, facilitant son utilisation sur Mac OS et Linux.

**Avantages de l'ex√©cution locale** :
- Contr√¥le des co√ªts : Pas besoin de payer pour des services de cloud computing co√ªteux.
- S√©curit√© des donn√©es : Les donn√©es restent sur votre machine, ce qui est crucial pour la confidentialit√©.

**Commandes simples** : Vous pouvez t√©l√©charger un mod√®le avec la commande ollama pull mistral, puis le lancer avec ollama run mistral. Une fois lanc√©, vous pouvez interagir avec le mod√®le via l'API REST.

**Large choix de mod√®les** : Ollama supporte plus de 60 mod√®les diff√©rents, y compris les populaires Llama 2, Mistral, Mixtral et Gemma.

**Efficacit√© des ressources** : Ollama utilise efficacement la m√©moire RAM pour ex√©cuter les mod√®les. Par exemple, un mod√®le 7B n√©cessite 8 GB de RAM, un mod√®le 13B n√©cessite 16 GB de RAM, et un mod√®le 33B n√©cessite 32 GB de RAM.

En r√©sum√©, Ollama est une solution pratique et efficace pour utiliser des mod√®les LLM en local, offrant simplicit√©, contr√¥le et s√©curit√©.

## Point important

### LangChain
LangChain est un cadre open source con√ßu pour faciliter la cr√©ation d'applications bas√©es sur de grands mod√®les de langage (LLM). Voici un aper√ßu de ses fonctionnalit√©s et avantages :

**Qu'est-ce qu'un LLM ?** : Les LLM sont des mod√®les de deep learning de grande taille, pr√©entra√Æn√©s sur d'√©normes quantit√©s de donn√©es. Ils peuvent g√©n√©rer des r√©ponses aux requ√™tes des utilisateurs, comme r√©pondre √† des questions ou cr√©er des images √† partir de descriptions textuelles.

**Outils et abstractions** : LangChain offre des outils et des abstractions pour am√©liorer la personnalisation, la pr√©cision et la pertinence des informations g√©n√©r√©es par les mod√®les. Cela permet aux d√©veloppeurs de cr√©er des applications plus sophistiqu√©es et adapt√©es aux besoins sp√©cifiques des utilisateurs.

**Cr√©ation et personnalisation** : Avec LangChain, les d√©veloppeurs peuvent facilement cr√©er de nouvelles cha√Ænes d'instructions ou personnaliser des mod√®les existants. Cela permet de tirer le meilleur parti des capacit√©s des LLM pour diverses applications.

### Le plus simple possible
```
# Faire de appel API Ollama simple, prend le CPU et est VRAIMENT lent
# -------------------------------------------------------------------
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate ## Custom Prompt
from langchain_core.output_parsers import StrOutputParser ## Transformer en String
llm = Ollama(model="llama3")

output_parser = StrOutputParser() ## Transforme le message de chat en string

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a world class technical documentation writer."),
    ("user", "{input}")
]) ## Guide pour manipuler la facon donc Ai reponds

chain = prompt | llm | output_parser ## Creation de la chain de commande

answer = chain.invoke({"input":"How can langsmith help with testing?"}) ## appel API de Ollama

print(answer)
# --------------------------------------------------------------------
```

### Faire des recherches sur le Net
```
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate ## Custom Prompt
from langchain_community.document_loaders import WebBaseLoader ## Charger une page internet pour interoger
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain

embeddings = OllamaEmbeddings()
llm = Ollama(model="llama3")
loader = WebBaseLoader("https://docs.smith.langchain.com/user_guide") # Charge la page
docs = loader.load()

# Separe le text en petit portion
text_splitter = RecursiveCharacterTextSplitter()

# variable pour sauvegarder la liste de portion
documents = text_splitter.split_documents(docs)

# garde le tout dans un vector pour l'AI
vector = FAISS.from_documents(documents, embeddings) 

prompt = ChatPromptTemplate.from_template(
"""Answer the following question based only on the provided context:

<context>
{context}
</context>

Question: {input}""") # custom prompt

# Creation de la chain d'instruction
document_chain = create_stuff_documents_chain(llm, prompt)

# Va chercher les infos pour l'AI
retriever = vector.as_retriever()

# On met tout ensemble
retrieval_chain = create_retrieval_chain(retriever,document_chain)

# Appel API
response = retrieval_chain.invoke({"input": "how can langsmith help with testing?"}) 
print(response["answer"])
```

**Acc√®s √† de nouvelle donn√©es** : LangChain inclut des composants qui permettent aux LLM d'acc√©der √† de nouvelle donn√©es sans n√©cessiter un nouvel entra√Ænement. Cela signifie que les mod√®les peuvent utiliser des informations actualis√©es ou sp√©cifiques sans devoir √™tre r√©entra√Æn√©s, ce qui √©conomise du temps et des ressources.

En r√©sum√©, LangChain est un outil puissant et flexible pour d√©velopper des applications bas√©es sur de grands mod√®les de langage, offrant des moyens innovants de personnaliser et d'am√©liorer les performances des mod√®les sans n√©cessiter de r√©entra√Ænement complet.

### Embeddings
Les embeddings sont une technique utilis√©e en intelligence artificielle et en traitement du langage naturel pour repr√©senter des donn√©es sous forme de vecteurs de nombres r√©els dans un espace de dimension inf√©rieure. Voici une explication d√©taill√©e de ce concept :

D√©finition :
Les embeddings sont des repr√©sentations vectorielles continues des objets (comme des mots, des phrases ou des images) qui pr√©servent les relations s√©mantiques ou contextuelles entre eux.

**Pourquoi les Embeddings ?** :
R√©duction de la Dimensionnalit√© : Les embeddings permettent de repr√©senter des donn√©es de haute dimension (comme des mots dans un vocabulaire) dans un espace de dimension inf√©rieure, ce qui facilite le traitement et le calcul.
Capturer les Relations S√©mantiques : Les vecteurs d'embeddings sont construits de mani√®re √† ce que des objets similaires soient proches les uns des autres dans l'espace vectoriel. Par exemple, les mots "roi" et "reine" auront des vecteurs similaires.

**Applications des Embeddings** :
- Traitement du Langage Naturel (NLP) : Les embeddings de mots (word embeddings) comme Word2Vec, GloVe, et les embeddings de phrase comme ceux g√©n√©r√©s par BERT, capturent les relations contextuelles entre les mots et les phrases.
- Repr√©sentation d'Images : Dans la vision par ordinateur, les embeddings d'images permettent de repr√©senter des images de mani√®re compacte tout en conservant les informations visuelles importantes.
- Recommandation : Les embeddings sont utilis√©s pour cr√©er des syst√®mes de recommandation en capturant les similarit√©s entre les utilisateurs et les produits.

**Types d'Embeddings** :
- Word Embeddings : Repr√©sentations vectorielles des mots. Exemples : Word2Vec, GloVe.
- Contextual Embeddings : Repr√©sentations des mots en tenant compte de leur contexte. Exemples : BERT, GPT.
- Document Embeddings : Repr√©sentations des phrases, paragraphes ou documents. Exemples : Doc2Vec, Sentence-BERT.

**Exemple Concret** :
Supposons que nous avons trois mots : "chat", "chien" et "voiture". Dans un espace de mots traditionnel (one-hot encoding), ces mots seraient repr√©sent√©s comme des vecteurs orthogonaux sans lien entre eux. En utilisant des embeddings, "chat" et "chien" auraient des vecteurs proches, car ils sont s√©mantiquement li√©s (tous deux sont des animaux), tandis que "voiture" serait plus √©loign√©.

**Comment sont-ils Appris** ? :
Les embeddings sont g√©n√©ralement appris √† partir de grandes quantit√©s de donn√©es √† l'aide de r√©seaux de neurones. Par exemple, Word2Vec utilise un r√©seau de neurones pour pr√©dire les mots environnants d'un mot cible (Skip-gram) ou pour pr√©dire un mot cible √† partir de ses mots environnants (CBOW).

En r√©sum√©, les embeddings sont des outils puissants en intelligence artificielle qui permettent de repr√©senter des donn√©es complexes de mani√®re compacte et significative, facilitant ainsi leur utilisation dans divers algorithmes et applications.

### Document Loader
Le Document Loader en Python est g√©n√©ralement utilis√© pour charger et traiter des documents de diff√©rentes sources et formats. Il peut √™tre une partie int√©grante de biblioth√®ques ou de frameworks qui manipulent des donn√©es textuelles. Voici un aper√ßu g√©n√©ral de son fonctionnement, souvent bas√© sur l'utilisation de biblioth√®ques comme LangChain, PyPDF2, Pandas, et autres.

### Chunking
Qu'est-ce que le Chunking ?

Le chunking est une technique en traitement du langage naturel (NLP) qui consiste √† diviser un texte en segments plus petits et significatifs appel√©s chunks. Ces chunks sont souvent des groupes de mots qui forment des unit√©s syntaxiques coh√©rentes, comme des syntagmes nominaux ou verbaux.

#### Chunking Ids
Chaque chunk peut √™tre identifi√© par un ID de chunk, qui sert √† r√©f√©rencer ou √† √©tiqueter ces segments de mani√®re unique et coh√©rente.
```
def calculate_chunk_ids(chunks):

    # This will create IDs like "data/monopoly.pdf:6:2"
    # Page Source : Page Number : Chunk Index

    last_page_id = None
    current_chunk_index = 0

    for chunk in chunks:
        source = chunk.metadata.get("source")
        page = chunk.metadata.get("page")
        current_page_id = f"{source}:{page}"

        # If the page ID is the same as the last one, increment the index.
        if current_page_id == last_page_id:
            current_chunk_index += 1
        else:
            current_chunk_index = 0

        # Calculate the chunk ID.
        chunk_id = f"{current_page_id}:{current_chunk_index}"
        last_page_id = current_page_id

        # Add it to the page meta-data.
        chunk.metadata["id"] = chunk_id

    return chunks
```
#### Chunking a la BD (SQLlite)
```
def add_to_chroma(chunks: list[Document]):
    # Load the existing database.
    db = Chroma(
        persist_directory=CHROMA_PATH, embedding_function=get_embedding_function()
    )

    # Calculate Page IDs.
    chunks_with_ids = calculate_chunk_ids(chunks)

    # Add or Update the documents.
    existing_items = db.get(include=[])  # IDs are always included by default
    existing_ids = set(existing_items["ids"])
    print(f"Number of existing documents in DB: {len(existing_ids)}")

    # Only add documents that don't exist in the DB.
    new_chunks = []
    for chunk in chunks_with_ids:
        if chunk.metadata["id"] not in existing_ids:
            new_chunks.append(chunk)

    if len(new_chunks):
        print(f"üëâ Adding new documents: {len(new_chunks)}")
        new_chunk_ids = [chunk.metadata["id"] for chunk in new_chunks]
        db.add_documents(new_chunks, ids=new_chunk_ids)
        db.persist()
    else:
        print("‚úÖ No new documents to add")
```
## Comment l'utiliser
```
def main():
 # Check if the database should be cleared (using the --clear flag).
    parser = argparse.ArgumentParser()
    parser.add_argument("--reset", action="store_true", help="Reset the database.")
    args = parser.parse_args()
    if args.reset:
        print("‚ú® Clearing Database")
        clear_database()

    # Create (or update) the data store.
    documents = load_documents()
    chunks = split_documents(documents)
    add_to_chroma(chunks)
```
## Chroma
Chroma est une biblioth√®que ou un outil souvent utilis√© en traitement du langage naturel (NLP) et en machine learning pour la gestion et la manipulation de vecteurs d'embeddings. Les embeddings sont des repr√©sentations vectorielles de donn√©es (comme des mots, des phrases, ou des documents) qui capturent les relations s√©mantiques entre ces donn√©es.

## Qu'est-ce qu'un rag?
RAG (Retrieval-Augmented Generation) est une technique en traitement du langage naturel (NLP) qui combine la g√©n√©ration de texte avec la r√©cup√©ration d'informations √† partir d'une base de donn√©es ou d'une source de connaissances externe. Cette m√©thode am√©liore la qualit√© et la pr√©cision des r√©ponses g√©n√©r√©es par des mod√®les de langage.
```
def query_rag(query_text: str):
    # Prepare the DB.
    embedding_function = get_embedding_function()
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)

    # Search the DB.
    results = db.similarity_search_with_score(query_text, k=5)

    context_text = "\n\n---\n\n".join([doc.page_content for doc, _score in results])
    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt = prompt_template.format(context=context_text, question=query_text)
    # print(prompt)

    model = Ollama(model="mistral")
    response_text = model.invoke(prompt)

    sources = [doc.metadata.get("id", None) for doc, _score in results]
    formatted_response = f"Response: {response_text}\nSources: {sources}"
    print(formatted_response)
    return response_text
```

Le RAG avec Ollama permet de cr√©er des syst√®mes de question-r√©ponse puissants et pr√©cis en combinant la r√©cup√©ration d'informations pertinentes avec la g√©n√©ration de texte enrichi par des mod√®les de langage. Cette technique tire parti des forces de chaque composant pour fournir des r√©ponses de haute qualit√© et pertinentes, en utilisant efficacement les capacit√©s locales de Ollama pour ex√©cuter des mod√®les LLM.
